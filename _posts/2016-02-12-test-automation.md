---
layout: post
title: Test Automation in 2016
categories:
  - test-automation
---

Test automation is a hot topic today. Projects are demanding faster feedback cycles and shorter release time frames. If even five years ago test automation was considered to be a luxury, nowadays most of the companies do it in one form or another or at least actively striving for it.

This switch is happening along with agile software methodology transformation phase. The agile approach is all about shortening change feedback cycle and producing business value more frequently. This requires testing tools to transform along with the development process. Some people use a concept of an "agile testing tool". When someone is referring agile testing tools, what they actually mean is – a tool that should support quick feedback cycles, test automation, painless inclusion in continuous integration lifecycle, and preferably have a liberal open-source license. This term was coined as an opposite to traditional one-stop commercial testing tools with internal proprietary scripting languages and limited or no integration possibilities with CI tools. Those tools would be focused on GUI testing as well, however as experience shows this type of test automation should be kept to a minimum.

<!--content-->

Industry veterans are agreeing that era of such tools is gone and testing space should now go the "microservice" way rather than picking the best monolith tool. In practice, it means that a company should concentrate on selecting best of the breed tools for every testing area rather than a single tool. Examples of such testing domain areas are test management, test reporting, test runner, test data management and others. Even though test automation is widely used now, there would be usually no, or very high-level testing tools strategy on a company level. Testing tool selection decisions are taken on individual project/department basis, which results in a heterogeneous testing environment. Furthermore, testing tools are dependent on the type of testing, deployment platform, and team preferences. Integration, GUI, browser, mobile, functional, security and performance tests all require various setups and tools.

There is a variety of open-source testing frameworks – Selenium, JUnit, TestNG, Robot Framework, Jasmin, JMeter, Gatling, Cucumber, Fitnesse to mention a few, plus plenty of niche commercial solutions. Some of the testing frameworks are not feature-complete and represent a test runner only (such as JUnit and TestNG). The test runner should be further accompanied by extensions and libraries to perform actual tasks. The selection of such extensions is largely dependent on use-cases being tested, protocols and technologies used, and so on. Since there is no separate purchase required, or you "pay as you go", it is quite easy for individual teams to introduce additional testing frameworks and libraries without consulting with overall company test tooling strategy.

Automated testing space is quite a tricky business and it is becoming even more fragmented with the rising popularity of various internal and public cloud-based testing solutions. Testing in the cloud is becoming the norm, especially if test activities are required to be performed on multiple consumer platforms, such as various browsers and mobile devices, not speaking of other smart gadgets appearing monthly. Instead of assembling own testing grid, which would be underutilized and idle for most of the time, teams are preferring to use cloud testing tools, which in turn use infrastructure from AWS and similar. Another common requirement for having your own company testing grid is shortening change feedback loops. It requires minimizing testing cycles, which is typically approached by running existing tests in parallel on a number of test agents, such as Selenium Grid. One problem with public testing grids, however, is security and network connectivity. Managing testing clouds adds even more complexity to overall company testing strategy.

There are very few open standards in test automation field, which would allow tools to cooperate with each other. The only standard, which seems to be widely accepted at the moment, is WebDriver communication protocol (https://www.w3.org/TR/webdriver/). WebDriver standard is still in a draft phase and describes a way how test automation client script could run test steps on remote user agents to utilize test grid capabilities. This approach is the foundation of Selenium Grid solutions and there is a variety of client and server implementations in different platforms and programming languages. As in other software development areas, creation and adoption of standards did result in mass interest of companies and individuals contributing to the ecosystem of interacting components. There are Selenium library implementations in every popular programming language and it could serve as an example of successful testing protocol standardization. Companies may be safe to pick such a tool because its future is not connected to the success of any particular vendor.

Test management tools were considered as something only viable for large software projects for a long time; plus most of the popular tools available were produced large software vendors for a steep license price. Test management tools would be historically oriented towards manual testing as well, either ignoring automated tests support completely or implementing such support as an afterthought. Such tools would need to be fundamentally re-engineered in order to meet the modern agile process requirements and will die off in time unless they do so.

Software testing and development practices currently keep merging with the advent of concepts like cross-functional teams, test-driven development, and the fact that developers are now responsible for testing and writing automated tests for their code as well. So a new generation lightweight test management practices started to appear. Instead of keeping test documentation in specialized web-based systems, software teams are starting to document test cases like source code (JavaDoc approach). In addition to plain documentation, it is possible to tag automated test cases with certain attributes like priority, traceability to a particular user story, test group, etc. This approach becomes a very lean version of what test management systems were supposed to provide.

The next step is to use a test result reporting tool, which would synchronize test records and tags between existing source code and some sort of visual representation. So the missing fragment of the puzzle is agile test results reporting tools. Most of the existing testing frameworks do support some sort of test reporting formats already, but those are not covering extra information such as requirement mapping and other associated test case information. Such reports were engineered for programmers as the target audience, which is why they ignore concepts from joint practices like requirement management and test management. Today, lightweight test reporting solutions are gaining popularity. Tools like Allure or Serenity (http://allure.qatools.ru/ and http://www.thucydides.info/) aim to pick up where xUnit testing tools family left the state of test result reporting. These tools would help you to organize test execution reports in a way that it makes sense for product owners, test managers and team leads, outlining functionality coverage and different sorts of metrics and result breakdowns. Still, existing open-source solutions are generating only snapshot reports for a single test run. These tools would not provide historical information, such as trend analysis, so this information is not available to support decision making. Looking at your historical test results unlocks new possibilities for test quality and coverage improvement and provides you with possibilities of comparing testing practices in your organization. Test management and test result reporting are both becoming the norm in a well-organized software development project of any size as tooling becomes available, approachable, and simple to use.

Integrating diverse testing tools and managing changes in the testing tools portfolio does require unified API definitions in order to treat results from different test frameworks uniformly and produce project-wide or company-wide aggregated reports at the end. Such a standard would allow gradual replacement of the tools without loss of control of your test tool strategy. JSON over HTTP is the most widely accepted protocol for services integration nowadays, which is often referred to as REST services. The architectural patterns and tooling have matured quite significantly over the recent years and are matching the state of SOA tools in their best days. REST services have standards both for payload and client-server interaction definition. The most interesting development so far is Open API initiative (https://openapis.org/) backed by companies like Google and IBM. It aims at providing a vendor-neutral format for REST API description and provides necessary tooling to generate client and server implementations in a variety of programming platforms.

We have identified a clear need for an open standard around test results submission. The problem domain is standard and mature enough across most of the popular frameworks and is not that complex to have major disagreements between tool vendors. Neueda, jointly with other test automation visionaries, is currently working towards defining a standard of gathering test results in a tool neutral way. This paves the road for tool vendors building test reporting engines and dashboards, which would work in compliance with the standard API specifications and create an open market for quality management solutions. Like with the Selenium WebDriver example mentioned before, this would enable testing framework and tool vendors writing adapters and plugins against the upcoming test reporting API. The common protocol will be accompanied by a number of common test framework adapters, which ensures easy integration of such tools.

Neueda offers a test reporting solution as well, which serves as a reference implementation for the unlocked capabilities and features coming from a standardized testing strategy. It is integrated with a variety of popular open-source testing tools and allows to see all of your test results at a glance.

Having a test reporting dashboard provides an opportunity to review the company-wide management-level testing situation. Similar tools are available in other software engineering domains. SonarQube (http://www.sonarqube.org/) open-source code quality management tool has become a de-facto standard recently. It gives you a bird's-eye view of code quality and unit testing coverage allowing to get comparable project statistics at a glance. You could drill-down to individual code lines and see all reported code problems. The same result could be achieved in test reporting space in the nearest future.

Such tools would allow to see company-level testing strategy execution in real time and see any deviations from the expected model. Gathering test reporting data, in the long run, helps to do a root cause analysis of problems encountered and focus the improvement and investment efforts. We, in Neueda, foresee that in 3 to 5 years test result reporting solutions will become the norm in the same way as other quality dashboards and common API will be the driving force behind it. Software development practices and tools are improving constantly. Projects are looking for ways to decrease expenses and make metric-based decisions, which the described solution is supporting.
